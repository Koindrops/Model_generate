import numpy as np
import pandas as pd
import pickle
import logging
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Concatenate
from tensorflow.keras.callbacks import CSVLogger, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.optimizers import Adam
from tcn import TCN

# Constants
SEQUENCE_LENGTH = 36
MODEL_PATH = "hybrid_tcn_model.h5"
SCALER_PATH = "scaler.pkl"
CSV_FILE_PATH = "block_data.csv"
BATCH_THRESHOLD = 100
EPOCHS = 10
BATCH_SIZE = 32

# Logging setup
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Fetch data and map to binary groups
def fetch_and_preprocess_data(file_path):
    try:
        data = pd.read_csv(file_path)
        required_columns = ["Serial Number", "Block Height", "Last Numerical Digit"]
        for col in required_columns:
            if col not in data.columns:
                raise ValueError(f"CSV file is missing required column: {col}.")
        
        last_digits = data["Last Numerical Digit"].dropna().astype(int).values
        binary_digits = np.where(last_digits < 5, 0, 1)
        return last_digits, binary_digits
    except Exception as e:
        logging.error(f"Error in fetching and preprocessing data: {str(e)}")
        raise

# Create sequences and labels
def create_sequences(data, binary_data, sequence_length):
    sequences, labels = [], []
    for i in range(len(data) - sequence_length):
        sequences.append(binary_data[i:i + sequence_length])
        labels.append(binary_data[i + sequence_length])
    return np.array(sequences), np.array(labels)

# Compute Markov transition probabilities
def compute_markov_features(binary_data, sequence_length):
    transition_matrix = np.zeros((2, 2))
    for i in range(1, len(binary_data)):
        transition_matrix[binary_data[i - 1], binary_data[i]] += 1
    transition_matrix /= transition_matrix.sum(axis=1, keepdims=True)
    markov_features = []
    for i in range(len(binary_data) - sequence_length):
        current_sequence = binary_data[i:i + sequence_length]
        probabilities = [transition_matrix[current_sequence[j], current_sequence[j + 1]] 
                         for j in range(len(current_sequence) - 1)]
        markov_features.append(np.mean(probabilities))
    return np.array(markov_features)

# Create a hybrid TCN model
def create_hybrid_tcn_model(sequence_length):
    sequence_input = Input(shape=(sequence_length, 1))
    tcn_output = TCN(64, activation="relu")(sequence_input)
    markov_input = Input(shape=(1,))
    combined = Concatenate()([tcn_output, markov_input])
    dense1 = Dense(32, activation="relu")(combined)
    output = Dense(1, activation="sigmoid")(dense1)
    model = Model(inputs=[sequence_input, markov_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.001), loss="binary_crossentropy", metrics=["accuracy"])
    return model

# Train and save the hybrid model
def train_and_save_hybrid_model():
    try:
        # Fetch and preprocess data
        last_digits, binary_digits = fetch_and_preprocess_data(CSV_FILE_PATH)
        scaler = MinMaxScaler(feature_range=(0, 1))
        scaled_data = scaler.fit_transform(last_digits.reshape(-1, 1))
        with open(SCALER_PATH, "wb") as scaler_file:
            pickle.dump(scaler, scaler_file)
        logging.info("Scaler saved successfully.")
        
        # Create sequences and labels
        X, y = create_sequences(scaled_data.flatten(), binary_digits, SEQUENCE_LENGTH)
        X = X.reshape((X.shape[0], X.shape[1], 1))
        markov_features = compute_markov_features(binary_digits, SEQUENCE_LENGTH)
        
        # Split data into training and validation sets
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
        markov_train, markov_val = train_test_split(markov_features, test_size=0.2, random_state=42)
        
        # Create the model
        model = create_hybrid_tcn_model(SEQUENCE_LENGTH)
        
        # Callbacks for monitoring and optimization
        csv_logger = CSVLogger("training_log.csv", append=True)
        lr_scheduler = ReduceLROnPlateau(monitor="loss", factor=0.5, patience=2, min_lr=1e-6, verbose=1)
        early_stopping = EarlyStopping(monitor="val_loss", patience=3, verbose=1, restore_best_weights=True)
        
        # Train the model
        history = model.fit(
            [X_train, markov_train], y_train,
            validation_data=([X_val, markov_val], y_val),
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            verbose=1,
            callbacks=[csv_logger, lr_scheduler, early_stopping]
        )
        
        # Save the trained model
        model.save(MODEL_PATH)
        logging.info(f"Hybrid model saved successfully as {MODEL_PATH}")
        
        # Save training history
        with open("training_history.json", "w") as history_file:
            import json
            # Convert all non-serializable types to Python native types
            history_serializable = {key: [float(value) for value in values] for key, values in history.history.items()}
            json.dump(history_serializable, history_file)
        logging.info("Training history saved successfully.")
    except Exception as e:
        logging.error(f"Error in training and saving the hybrid model: {str(e)}")
        raise

# Run the training process
if __name__ == "__main__":
    train_and_save_hybrid_model()
